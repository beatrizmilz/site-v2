---
title: "Movimento da faxina de dados"
date: "2019-07-31"
tags: ["golem", "shiny"]
categories: ["tutoriais", "r"]
banner: "img/banners/golem.png"
author: ["Julio"]
summary: "A faxina de dados é uma etapa essencial do ciclo da ciência de dados, que nunca será automatizada. Seja parte desse movimento."
draft: false
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, out.width = "50%")
```

Fazer a faxina/normalização/ETL/outros nomes é a etapa mais trabalhosa do ciclo da ciência de dados. Para mim, essa é a parte mais difícil de automatizar do nosso trabalho, pois está intimamente relacionada com o problema de negócio. O seu modelo fantástico de florestas profundas ou redes aleatórias (pun intended) nada fará, ou fará coisa errada, se sua variável resposta não estiver bem montada. O seu gráfico de pizza que faz *voosh* não impressionará ninguém se suas categorias não estiverem padronizadas. E por aí vai.

Várias pesquisas mostram que o estatístico passa boa parte de seu tempo arrumando dados. E isso é passado como uma das dores da carreira. Mas quem disse que a faxina precisa ser chata? 

Para mim, nada dá mais prazer do que tornar uma base de dados pública suja em algo simples de consumir! Mais do que fornecer respostas prontas, a faxina de dados democratiza a ciência de dados, pois permite que outras pessoas realizem análises com as bases montadas.


## Princípios da faxina de dados

1. Reprodutibilidade é essencial.
2. O trade-off entre tempo investido e qualidade da base não é linear.
3. 


## O ciclo da faxina de dados


...

Nesse texto, mostro como fazer a faxina de uma base de dados bem complicada

```{r}
library(tidyverse)
library(lubridate)

d_bach <- read_csv("~/Downloads/bacharel_010819.csv") %>% 
  janitor::clean_names() %>% 
  mutate(
    dt_nasc = if_else(dt_nasc == "00/00/0000", NA_character_, dt_nasc),
    dt_nasc = dmy(dt_nasc),
    status = toupper(status),
    status = if_else(str_detect(status, "BAIX|SUSP"), "BAIXADO", status),
    idade = interval(dt_nasc, Sys.Date()) / duration(num = 1, units = "years")
  ) %>% 
  filter(status %in% c(NA_character_, "ATIVO"), is.na(idade) | idade > 0)

glimpse(d_bach)

d_bach %>% 
  filter(conre == "CONRE 3", idade > 90) %>% 
  select(nome, cpf) %>% 
  arrange(nome) %>% 
  knitr::kable()

```

```{r}
library(ggridges)

d_bach %>% 
  mutate(conre = fct_reorder(conre, idade)) %>% 
  ggplot() +
  geom_density_ridges(aes(x = idade, y = conre), 
                      quantile_lines = TRUE, quantiles = 2,
                      bandwidth = 3.6, alpha = .6, 
                      fill = "royalblue") +
  scale_x_continuous(limits = c(0, NA), breaks = 0:10 * 12) +
  theme_minimal(16) +
  ggtitle("Distribuição das idades", 
          "Estatísticos ativos em cada CONRE")
```


```{r}
d_bach %>% 
  group_by(conre) %>% 
  summarise(`> 80` = sum(idade > 80), 
            `> 90` = sum(idade > 90),
            `> 100` = sum(idade > 100)) %>% 
  knitr::kable()
```




É isso. Happy coding ;)









